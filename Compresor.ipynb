{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc5PwS2SmpHC"
      },
      "source": [
        "ЛАБОРАТОРНА РОБОТА 1\n",
        "====================================================\n",
        "Стиснення тексту за допомогою генеративної моделі\n",
        "=====\n",
        "Автори: Баклан Аліса, Кіндякова Діана, Віткіна Анна\n",
        "Група МІ-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z9dHXrlakm2",
        "outputId": "d3d2b1ed-e4d0-42bb-ebd0-9eaa439a9792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "ERROR: unknown command \"torch\"\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oViIR_JQntg6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import AlbertTokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "PREDICT_WINDOW = 10000\n",
        "NUM_WORDS = 100\n",
        "SHOW_TOKENS = False\n",
        "SHOW_PREDICTION = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARe_ELdyfaH2"
      },
      "outputs": [],
      "source": [
        "class TextCompressor:\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "    def compress(self, input_text):\n",
        "        compressed_tokens = []\n",
        "        current_text = \"\"\n",
        "\n",
        "        # Use regular expression to split the input text into tokens with spaces as separators\n",
        "        pattern = r'\\d+|[а-яА-ЯїЇєЄіIґҐ]+\\'?[а-яА-ЯїЇєЄіIґҐ]*|[^а-яА-Я0-9їЇєЄіIґҐ]'\n",
        "        no_letter_or_didgit_pattern = r'[^а-яА-Я0-9їЇєЄіIґҐ]'\n",
        "        tokens = re.findall(pattern, input_text)\n",
        "        if SHOW_TOKENS:\n",
        "            print(tokens)\n",
        "\n",
        "        if tokens:\n",
        "            # Handle the first word separately\n",
        "            first_word = tokens[0]\n",
        "            compressed_tokens.append(first_word)\n",
        "            current_text += first_word\n",
        "            tokens = tokens[1:]\n",
        "\n",
        "        for token in tokens:\n",
        "            if re.fullmatch(no_letter_or_didgit_pattern, token):\n",
        "                # If the token is spaces or a special symbol, append it as is\n",
        "                compressed_tokens.append(token)\n",
        "            elif token.isdigit():\n",
        "                # If the token is a number, simply append it with a prefix \"nu\"\n",
        "                compressed_tokens.append(\"<n\" + token+\">\")\n",
        "            else:\n",
        "                # Predict the next words based on the current text\n",
        "                prediction = self.predict_next(current_text, NUM_WORDS)\n",
        "                if SHOW_PREDICTION:\n",
        "                        print(prediction)\n",
        "\n",
        "                if token in prediction:\n",
        "                    # If the token is predicted, append its index as a number\n",
        "                    index = prediction.index(token)\n",
        "                    compressed_tokens.append(str(index))\n",
        "                    if SHOW_PREDICTION:\n",
        "                        print(index)\n",
        "                else:\n",
        "                    # If the token is not predicted, append it as is\n",
        "                    compressed_tokens.append(token)\n",
        "                    if SHOW_PREDICTION:\n",
        "                        print(token)\n",
        "\n",
        "            # Update the current_text\n",
        "            current_text += token\n",
        "\n",
        "        # Join the compressed tokens into a single string\n",
        "        compressed_text = \"\".join(compressed_tokens)\n",
        "        return compressed_text\n",
        "\n",
        "    def decompress(self, compressed_text):\n",
        "        decompressed_tokens = []\n",
        "        current_text = \"\"\n",
        "        add_number = False  # Flag to indicate the next number should be added as it is\n",
        "\n",
        "        # Use regular expression to split the compressed text into tokens\n",
        "        pattern = r'<n\\d+>|\\d+|[а-яА-ЯїЇєЄіIґҐ]+\\'?[а-яА-ЯїЇєЄіIґҐ]*|[^а-яА-Я0-9їЇєЄіIґҐ]'\n",
        "        number_pattern = r'<n\\d+>'\n",
        "        tokens = re.findall(pattern, compressed_text)\n",
        "        if SHOW_TOKENS:\n",
        "            print(tokens)\n",
        "\n",
        "        for token in tokens:\n",
        "            if re.fullmatch(number_pattern, token):\n",
        "                # If the token matches the number pattern add it as a regular number\n",
        "                token = token[2:len(token)-1]\n",
        "                decompressed_tokens.append(token)\n",
        "            elif token.isdigit():\n",
        "                # Convert the number to an integer\n",
        "                word_index = int(token)\n",
        "                # Predict the next words based on the current text\n",
        "                prediction = self.predict_next(current_text, NUM_WORDS)\n",
        "\n",
        "                # Use the predicted word at the specified index\n",
        "                if word_index < len(prediction):\n",
        "                    word = prediction[word_index]\n",
        "                    # Check if the word is a special character and adjust spacing accordingly\n",
        "                    decompressed_tokens.append(word)\n",
        "                else:\n",
        "                    # If the index is out of range, append the number itself\n",
        "                    decompressed_tokens.append(token)\n",
        "            else:\n",
        "                # If the token is not a number, simply append it to the current text\n",
        "                decompressed_tokens.append(token)\n",
        "\n",
        "            # Update the current_text\n",
        "            current_text = \"\".join(decompressed_tokens)\n",
        "\n",
        "        # Join the decompressed tokens into a single string\n",
        "        decompressed_text = \"\".join(decompressed_tokens)\n",
        "        return decompressed_text\n",
        "\n",
        "    def predict_next(self, input_text, num_words):\n",
        "        input_text = input_text[-PREDICT_WINDOW:]\n",
        "        input_ids = self.tokenizer.encode(input_text, add_special_tokens=False, return_tensors='pt')\n",
        "\n",
        "        # Generate probabilities for the next word\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids).logits\n",
        "\n",
        "        # Get the last token's probabilities\n",
        "        next_word_logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the top num_words words with the highest probabilities\n",
        "        top_next_word_indices = torch.topk(next_word_logits, k=num_words, dim=-1).indices[0]\n",
        "        top_next_words = [self.tokenizer.decode(idx.item()) for idx in top_next_word_indices]\n",
        "        if SHOW_PREDICTION:\n",
        "            print(top_next_words)\n",
        "        return top_next_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAWftNEKP1Qt",
        "outputId": "b2ecbe47-87cb-4c42-b0dd-697dbf843d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>>decompress\n",
            "Enter a file name: (compressed)test.txt\n",
            "Input text:  Верблюд — 6 жвачний ссавець ряду парнокопитних. 2 2 0 0 жвачних верблюд має трьохкамерний 87. Має 5 9 2 30 9 85 33’1 підошвою, пристосованою 1 51 0 3, 0 копита, 10 0 нігті.\n",
            "Тварини тривалий 0 35 обходитися 0 0, 1 6 випаде 0 знайти 7 6 колючу рослину (16 29 0 1 — верблюжа колючка), 0 76 0 зжує, 5 їхній 17 нечутливий 0 уколів 2 захищений 0 0. Верблюди довго можуть обходитися 0 1, 24 13 натраплять, 0 випивають 22 <n10> 1 2 <n130> літрів. Темного 0 16 82 57 15, 0 27 4 0 32 <n1> кілометру. Довга пухнаста вовна 94 44 0 перегрівання 9 палючим 0 7.\n",
            "Decompressed text:  Верблюд — великий жвачний ссавець ряду парнокопитних. На відміну від інших жвачних верблюд має трьохкамерний шлунок. Має два пальці на нозі з широкою м’якою підошвою, пристосованою для ходіння по піску, і копита, схожі на нігті.\n",
            "Тварини тривалий час можуть обходитися без їжі, а якщо випаде нагода знайти в пустелі колючу рослину (вона так і називається — верблюжа колючка), то легко її зжує, бо їхній рот нечутливий до уколів і захищений від них. Верблюди довго можуть обходитися без води, зате коли натраплять, то випивають за 10 хвилин до 130 літрів. Темного кольору очі здатні помітити людину, яка рухається на відстані до 1 кілометру. Довга пухнаста вовна захищає тварину від перегрівання під палючим сонцем пустелі.\n",
            ">>>stop\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "model_name = \"Tereveni-AI/gpt2-124M-uk-fiction\"\n",
        "text_compressor = TextCompressor(model_name)\n",
        "\n",
        "while (True):\n",
        "    command = input('>>>')\n",
        "    if command == \"stop\":\n",
        "        break\n",
        "    if command == \"help\":\n",
        "        print(\"stop\\ncompress file_name\\ndecompress file_name\")\n",
        "        continue\n",
        "    filename = input('Enter a file name: ')\n",
        "    try:\n",
        "        in_file = open(filename, 'r')\n",
        "    except FileNotFoundError:\n",
        "        print(\"FileNotFoundError: No such file or directory\", filename)\n",
        "        continue\n",
        "    except PermissionError:\n",
        "        print(\"Permission Denied Error: Access is denied\")\n",
        "        continue\n",
        "    else:\n",
        "        input_text = in_file.read()\n",
        "        print(\"Input text: \", input_text)\n",
        "    finally:\n",
        "        in_file.close()\n",
        "\n",
        "    input_size = len(input_text)\n",
        "    if command == \"compress\":\n",
        "        with open(\"(compressed)\" + filename, 'w') as out_file:\n",
        "            compressed_text = text_compressor.compress(input_text)\n",
        "            print(\"Compressed text: \", compressed_text)\n",
        "            out_file.write(compressed_text)\n",
        "            output_size = len(compressed_text)\n",
        "            print(\"Compressing status: {:%}\".format((input_size - output_size)/input_size))\n",
        "\n",
        "    if command == \"decompress\":\n",
        "        with open(\"(decompressed)\" + filename, 'w') as out_file:\n",
        "            decompressed_text = text_compressor.decompress(input_text)\n",
        "            print(\"Decompressed text: \", decompressed_text)\n",
        "            out_file.write(decompressed_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}